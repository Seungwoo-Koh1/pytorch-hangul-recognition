{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112d94dd-8c08-445e-a0e5-5329338197f6",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2ba32c-3122-4551-9674-6a72a326dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *------- Basic setup -------*\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random, time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# *------- torch -------*\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# *------- albumentations -------*\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# *------- sklearn -------*\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29fe47-6cb9-4141-b4e6-1d8fa6c0ccc7",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1027188-9b09-48f9-998d-371a2ee3e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 256\n",
    "NUM_EPOCHS = 100\n",
    "NUM_CPU = cpu_count()\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a9806-eb03-4910-ac16-3092c97c6788",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61804326-c135-4b2e-a689-3853af94ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(csv_path, train_save_path, valid_save_path, test_size=0.2, random_seed=SEED):\n",
    "    \"\"\"\n",
    "    주어진 CSV 파일을 train과 val 데이터셋으로 나누어 저장하는 함수.\n",
    "\n",
    "    Args:\n",
    "    - csv_path: 입력 CSV 파일 경로\n",
    "    - train_save_path: 학습 데이터셋 저장 경로\n",
    "    - valid_save_path: 검증 데이터셋 저장 경로\n",
    "    - test_size: 검증 데이터셋의 비율 (default: 0.2)\n",
    "    - random_seed: 데이터 분할시 사용되는 랜덤 시드 (default: 42)\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(csv_path, header=None, names=['path', 'label'])\n",
    "\n",
    "    # 데이터를 train과 val로 나누기\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=random_seed, stratify=df['label'])\n",
    "\n",
    "    # 나눠진 데이터를 CSV 파일로 저장\n",
    "    train_df.to_csv(train_save_path, index=False, header=False)\n",
    "    valid_df.to_csv(valid_save_path, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbf93f1d-88f8-445b-a500-3dcbdeec5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset('image-data/labels-map.csv', 'image-data/train-labels.csv', 'image-data/valid-labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a57f0-19aa-404f-8d12-6ba5b064d5bc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc335906-8938-4e3c-ad86-020589f97304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms():\n",
    "    return A.Compose([\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.OneOf([A.Rotate(limit=10),\n",
    "                 A.RandomBrightness(),\n",
    "                 A.CoarseDropout(),\n",
    "                 A.Cutout(num_holes=8, max_h_size=1, max_w_size=1, fill_value=1),\n",
    "                 ], p=1.0),\n",
    "        ToTensorV2(p=1.0)\n",
    "    ])\n",
    "def get_valid_transforms():\n",
    "    return ToTensorV2(p=1.0)\n",
    "\n",
    "def get_inferecne_transforms():\n",
    "    return ToTensorV2(p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9644491c-153d-4f8d-a826-f0b9eff338ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanHandwritingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    한글 손글씨 데이터셋을 로드하고 처리하기 위한 클래스.\n",
    "\n",
    "    Attributes:\n",
    "    - dataset (DataFrame): CSV 파일에서 읽어들인 데이터.\n",
    "    - root_dir (str): 이미지 파일이 저장된 기본 디렉터리 경로.\n",
    "    - transform (callable, optional): 샘플에 적용될 변환 (예: 데이터 증강).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, label_file, transforms=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - csv_file (str): CSV 파일의 경로.\n",
    "        - root_dir (str): 모든 이미지가 저장된 디렉터리 경로.\n",
    "        - label_file (str) :\n",
    "        - transform (callable, optional): 샘플에 적용할 선택적 변환.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file, header=None, names=['path', 'label'])\n",
    "        self.root_dir = root_dir\n",
    "        self.label_file = label_file\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # 파일에서 한글 글자를 읽어오기\n",
    "        with open(self.label_file, 'r', encoding='utf-8') as f:\n",
    "            hangul_chars = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        # 각 한글 글자에 순차적으로 레이블 번호 부여\n",
    "        self.label_mapping = {char: idx for idx, char in enumerate(hangul_chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 길이를 반환합니다.\n",
    "        \n",
    "        Returns:\n",
    "        - int: 데이터셋 내의 샘플 수.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        주어진 인덱스에 해당하는 샘플을 반환합니다.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx (int): 반환할 샘플의 인덱스.\n",
    "\n",
    "        Returns:\n",
    "        - dict: 'image' 및 'label' 키를 포함하는 사전.\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.root_dir, self.dataset.iloc[idx]['path'])\n",
    "        # Gray 이미지를 RGB로 열기\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img /= 255\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transforms(image=img)['image']\n",
    "                \n",
    "        label = self.dataset.iloc[idx]['label']\n",
    "        label = self.label_mapping[label]\n",
    "        label = torch.tensor(label, dtype=torch.int64)\n",
    "        return img, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed4a1957-88c2-4e0a-8fb6-fe103a5abf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18022 4506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeongseon/GitHub/pytorch-hangul-recognition/venv/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:1258: FutureWarning: This class has been deprecated. Please use RandomBrightnessContrast\n",
      "  warnings.warn(\n",
      "/home/yeongseon/GitHub/pytorch-hangul-recognition/venv/lib/python3.10/site-packages/albumentations/augmentations/dropout/cutout.py:49: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "TRAIN_CSV_FILE = \"./image-data/train-labels.csv\"\n",
    "VALID_CSV_FILE = \"./image-data/valid-labels.csv\"\n",
    "LABEL_FILE = \"./labels/256-common-hangul.txt\"\n",
    "ROOT_DIR = \"./image-data/hangul-images\"\n",
    "\n",
    "train_dataset = KoreanHandwritingDataset(TRAIN_CSV_FILE, ROOT_DIR, LABEL_FILE, get_train_transforms())\n",
    "valid_dataset = KoreanHandwritingDataset(VALID_CSV_FILE, ROOT_DIR, LABEL_FILE, get_valid_transforms())\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "valid_size = len(valid_dataset)\n",
    "print(train_size, valid_size)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers = NUM_CPU)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=False, num_workers = NUM_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3be92f1-a4ed-4e9b-a46b-d92afa849af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeongseon/GitHub/pytorch-hangul-recognition/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yeongseon/GitHub/pytorch-hangul-recognition/venv/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [64, 64, 32, 32]           9,408\n",
      "       BatchNorm2d-2           [64, 64, 32, 32]             128\n",
      "              ReLU-3           [64, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [64, 64, 16, 16]               0\n",
      "            Conv2d-5           [64, 64, 16, 16]          36,864\n",
      "       BatchNorm2d-6           [64, 64, 16, 16]             128\n",
      "              ReLU-7           [64, 64, 16, 16]               0\n",
      "            Conv2d-8           [64, 64, 16, 16]          36,864\n",
      "       BatchNorm2d-9           [64, 64, 16, 16]             128\n",
      "             ReLU-10           [64, 64, 16, 16]               0\n",
      "       BasicBlock-11           [64, 64, 16, 16]               0\n",
      "           Conv2d-12           [64, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-13           [64, 64, 16, 16]             128\n",
      "             ReLU-14           [64, 64, 16, 16]               0\n",
      "           Conv2d-15           [64, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-16           [64, 64, 16, 16]             128\n",
      "             ReLU-17           [64, 64, 16, 16]               0\n",
      "       BasicBlock-18           [64, 64, 16, 16]               0\n",
      "           Conv2d-19           [64, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-20           [64, 64, 16, 16]             128\n",
      "             ReLU-21           [64, 64, 16, 16]               0\n",
      "           Conv2d-22           [64, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-23           [64, 64, 16, 16]             128\n",
      "             ReLU-24           [64, 64, 16, 16]               0\n",
      "       BasicBlock-25           [64, 64, 16, 16]               0\n",
      "           Conv2d-26            [64, 128, 8, 8]          73,728\n",
      "      BatchNorm2d-27            [64, 128, 8, 8]             256\n",
      "             ReLU-28            [64, 128, 8, 8]               0\n",
      "           Conv2d-29            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-30            [64, 128, 8, 8]             256\n",
      "           Conv2d-31            [64, 128, 8, 8]           8,192\n",
      "      BatchNorm2d-32            [64, 128, 8, 8]             256\n",
      "             ReLU-33            [64, 128, 8, 8]               0\n",
      "       BasicBlock-34            [64, 128, 8, 8]               0\n",
      "           Conv2d-35            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-36            [64, 128, 8, 8]             256\n",
      "             ReLU-37            [64, 128, 8, 8]               0\n",
      "           Conv2d-38            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-39            [64, 128, 8, 8]             256\n",
      "             ReLU-40            [64, 128, 8, 8]               0\n",
      "       BasicBlock-41            [64, 128, 8, 8]               0\n",
      "           Conv2d-42            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-43            [64, 128, 8, 8]             256\n",
      "             ReLU-44            [64, 128, 8, 8]               0\n",
      "           Conv2d-45            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-46            [64, 128, 8, 8]             256\n",
      "             ReLU-47            [64, 128, 8, 8]               0\n",
      "       BasicBlock-48            [64, 128, 8, 8]               0\n",
      "           Conv2d-49            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-50            [64, 128, 8, 8]             256\n",
      "             ReLU-51            [64, 128, 8, 8]               0\n",
      "           Conv2d-52            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-53            [64, 128, 8, 8]             256\n",
      "             ReLU-54            [64, 128, 8, 8]               0\n",
      "       BasicBlock-55            [64, 128, 8, 8]               0\n",
      "           Conv2d-56            [64, 256, 4, 4]         294,912\n",
      "      BatchNorm2d-57            [64, 256, 4, 4]             512\n",
      "             ReLU-58            [64, 256, 4, 4]               0\n",
      "           Conv2d-59            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-60            [64, 256, 4, 4]             512\n",
      "           Conv2d-61            [64, 256, 4, 4]          32,768\n",
      "      BatchNorm2d-62            [64, 256, 4, 4]             512\n",
      "             ReLU-63            [64, 256, 4, 4]               0\n",
      "       BasicBlock-64            [64, 256, 4, 4]               0\n",
      "           Conv2d-65            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-66            [64, 256, 4, 4]             512\n",
      "             ReLU-67            [64, 256, 4, 4]               0\n",
      "           Conv2d-68            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-69            [64, 256, 4, 4]             512\n",
      "             ReLU-70            [64, 256, 4, 4]               0\n",
      "       BasicBlock-71            [64, 256, 4, 4]               0\n",
      "           Conv2d-72            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-73            [64, 256, 4, 4]             512\n",
      "             ReLU-74            [64, 256, 4, 4]               0\n",
      "           Conv2d-75            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-76            [64, 256, 4, 4]             512\n",
      "             ReLU-77            [64, 256, 4, 4]               0\n",
      "       BasicBlock-78            [64, 256, 4, 4]               0\n",
      "           Conv2d-79            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-80            [64, 256, 4, 4]             512\n",
      "             ReLU-81            [64, 256, 4, 4]               0\n",
      "           Conv2d-82            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-83            [64, 256, 4, 4]             512\n",
      "             ReLU-84            [64, 256, 4, 4]               0\n",
      "       BasicBlock-85            [64, 256, 4, 4]               0\n",
      "           Conv2d-86            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-87            [64, 256, 4, 4]             512\n",
      "             ReLU-88            [64, 256, 4, 4]               0\n",
      "           Conv2d-89            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-90            [64, 256, 4, 4]             512\n",
      "             ReLU-91            [64, 256, 4, 4]               0\n",
      "       BasicBlock-92            [64, 256, 4, 4]               0\n",
      "           Conv2d-93            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-94            [64, 256, 4, 4]             512\n",
      "             ReLU-95            [64, 256, 4, 4]               0\n",
      "           Conv2d-96            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-97            [64, 256, 4, 4]             512\n",
      "             ReLU-98            [64, 256, 4, 4]               0\n",
      "       BasicBlock-99            [64, 256, 4, 4]               0\n",
      "          Conv2d-100            [64, 512, 2, 2]       1,179,648\n",
      "     BatchNorm2d-101            [64, 512, 2, 2]           1,024\n",
      "            ReLU-102            [64, 512, 2, 2]               0\n",
      "          Conv2d-103            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-104            [64, 512, 2, 2]           1,024\n",
      "          Conv2d-105            [64, 512, 2, 2]         131,072\n",
      "     BatchNorm2d-106            [64, 512, 2, 2]           1,024\n",
      "            ReLU-107            [64, 512, 2, 2]               0\n",
      "      BasicBlock-108            [64, 512, 2, 2]               0\n",
      "          Conv2d-109            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-110            [64, 512, 2, 2]           1,024\n",
      "            ReLU-111            [64, 512, 2, 2]               0\n",
      "          Conv2d-112            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-113            [64, 512, 2, 2]           1,024\n",
      "            ReLU-114            [64, 512, 2, 2]               0\n",
      "      BasicBlock-115            [64, 512, 2, 2]               0\n",
      "          Conv2d-116            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-117            [64, 512, 2, 2]           1,024\n",
      "            ReLU-118            [64, 512, 2, 2]               0\n",
      "          Conv2d-119            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-120            [64, 512, 2, 2]           1,024\n",
      "            ReLU-121            [64, 512, 2, 2]               0\n",
      "      BasicBlock-122            [64, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-123            [64, 512, 1, 1]               0\n",
      "          Linear-124                  [64, 256]         131,328\n",
      "================================================================\n",
      "Total params: 21,416,000\n",
      "Trainable params: 21,416,000\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 503.38\n",
      "Params size (MB): 81.70\n",
      "Estimated Total Size (MB): 588.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 사전학습된 가중치를 가져오지 않도록 pretrained는 False\n",
    "model = models.resnet34(pretrained=False)\n",
    "\n",
    "# number of features in the input of the linear layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# sets the number of features of the linear layer\n",
    "model.fc = torch.nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# parameters\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "summary(model, (3, IMG_SIZE[0], IMG_SIZE[1]), BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a96563a-672b-421e-b77a-aa8074442dda",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# 모델 학습 함수 정의\n",
    "def train_model(model, criterion, optimizer, num_epochs, train_loader, val_loader, save_dir='model_best.pth'):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        \n",
    "        # 학습 모드 설정\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for step, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"train\")):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        # 검증 모드 설정\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"valid\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / valid_size\n",
    "        epoch_acc = running_corrects.double() / valid_size\n",
    "        print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        print('-' * 30)\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            # 최적의 모델 가중치 저장\n",
    "            torch.save(best_model, save_dir)\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val Acc: {:.4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "# train the model\n",
    "model = train_model(model, criterion, optimizer, NUM_EPOCHS, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce33ba3c-522a-4781-b6c5-dde45f4b367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_image(model, image_path, transform):\n",
    "    # 이미지 불러오기\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # 이미지 전처리\n",
    "    image = transform(image).unsqueeze(0) # 차원 추가 (배치 차원)\n",
    "    image = image.to(DEVICE)\n",
    "    \n",
    "    # 모델 추론 모드 설정 및 예측\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.item()\n",
    "\n",
    "# ResNet34의 입력 사이즈와 일치하도록 이미지 전처리를 위한 transform 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),  # ResNet34 입력 사이즈에 맞춰 이미지 크기 조정\n",
    "    transforms.ToTensor(),\n",
    "    # 만약 추가 전처리가 필요하면 이곳에 추가\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901d412d-d1cb-46c8-9699-b08b92b56057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 127\n"
     ]
    }
   ],
   "source": [
    "# 예측 수행\n",
    "image_path = './test-images/test_1.jpeg'\n",
    "predicted_label = predict_image(model, image_path, transform)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c2735bd-8314-40fa-8813-cf171ae67372",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './test-images/test_2.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test-images/test_2.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m predicted_label \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Label: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m, in \u001b[0;36mpredict_image\u001b[0;34m(model, image_path, transform)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_image\u001b[39m(model, image_path, transform):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# 이미지 불러오기\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# 이미지 전처리\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     image \u001b[38;5;241m=\u001b[39m transform(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# 차원 추가 (배치 차원)\u001b[39;00m\n",
      "File \u001b[0;32m~/GitHub/pytorch-hangul-recognition/venv/lib/python3.10/site-packages/PIL/Image.py:3227\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3224\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[1;32m   3226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3227\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3228\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3230\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './test-images/test_2.jpg'"
     ]
    }
   ],
   "source": [
    "image_path = './test-images/test_2.jpg'\n",
    "predicted_label = predict_image(model, image_path, transform)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc7d4d35-96de-4222-a754-79cff3dd35a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 214\n"
     ]
    }
   ],
   "source": [
    "image_path = './test-images/test_3.jpg'\n",
    "predicted_label = predict_image(model, image_path, transform)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd0907-7941-4951-a57e-16128bee9cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
