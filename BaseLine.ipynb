{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112d94dd-8c08-445e-a0e5-5329338197f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2ba32c-3122-4551-9674-6a72a326dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *------- Basic setup -------*\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random, time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# *------- torch -------*\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# *------- albumentations -------*\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# *------- sklearn -------*\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29fe47-6cb9-4141-b4e6-1d8fa6c0ccc7",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1027188-9b09-48f9-998d-371a2ee3e61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 256\n",
    "NUM_EPOCHS = 20\n",
    "NUM_CPU = cpu_count()\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a9806-eb03-4910-ac16-3092c97c6788",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61804326-c135-4b2e-a689-3853af94ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(csv_path, train_save_path, valid_save_path, test_size=0.2, random_seed=SEED):\n",
    "    \"\"\"\n",
    "    주어진 CSV 파일을 train과 val 데이터셋으로 나누어 저장하는 함수.\n",
    "\n",
    "    Args:\n",
    "    - csv_path: 입력 CSV 파일 경로\n",
    "    - train_save_path: 학습 데이터셋 저장 경로\n",
    "    - valid_save_path: 검증 데이터셋 저장 경로\n",
    "    - test_size: 검증 데이터셋의 비율 (default: 0.2)\n",
    "    - random_seed: 데이터 분할시 사용되는 랜덤 시드 (default: 42)\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(csv_path, header=None, names=['path', 'label'])\n",
    "\n",
    "    # 데이터를 train과 val로 나누기\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=random_seed, stratify=df['label'])\n",
    "\n",
    "    # 나눠진 데이터를 CSV 파일로 저장\n",
    "    train_df.to_csv(train_save_path, index=False, header=False)\n",
    "    valid_df.to_csv(valid_save_path, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbf93f1d-88f8-445b-a500-3dcbdeec5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset('image-data/labels-map.csv', 'image-data/train-labels.csv', 'image-data/valid-labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a57f0-19aa-404f-8d12-6ba5b064d5bc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9644491c-153d-4f8d-a826-f0b9eff338ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanHandwritingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    한글 손글씨 데이터셋을 로드하고 처리하기 위한 클래스.\n",
    "\n",
    "    Attributes:\n",
    "    - dataset (DataFrame): CSV 파일에서 읽어들인 데이터.\n",
    "    - root_dir (str): 이미지 파일이 저장된 기본 디렉터리 경로.\n",
    "    - transform (callable, optional): 샘플에 적용될 변환 (예: 데이터 증강).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - csv_file (str): CSV 파일의 경로.\n",
    "        - root_dir (str): 모든 이미지가 저장된 디렉터리 경로.\n",
    "        - transform (callable, optional): 샘플에 적용할 선택적 변환.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file, header=None, names=['path', 'label'])\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 길이를 반환합니다.\n",
    "        \n",
    "        Returns:\n",
    "        - int: 데이터셋 내의 샘플 수.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        주어진 인덱스에 해당하는 샘플을 반환합니다.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx (int): 반환할 샘플의 인덱스.\n",
    "\n",
    "        Returns:\n",
    "        - dict: 'image' 및 'label' 키를 포함하는 사전.\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.root_dir, self.dataset.iloc[idx]['path'])\n",
    "        # Gray 이미지를 RGB로 열기\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        #label = self.dataset.iloc[idx]['label']\n",
    "        label = ord(label)  # 유니코드 값으로 변환\n",
    "        \n",
    "        sample = {'image': image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f55fb561-cbee-443d-9e93-1ef7e991c11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Resize\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Sample의 'image'와 'label'을 PyTorch Tensor로 변환합니다.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_size=(64, 64)):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "        self.resizer = Resize(self.output_size)\n",
    "        \n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        sample로부터 'image'와 'label'을 추출하여 Tensor로 변환합니다.\n",
    "        \n",
    "        Args:\n",
    "        - sample (dict): 'image'와 'label' key를 가진 사전\n",
    "        \n",
    "        Returns:\n",
    "        - dict: 변환된 'image'와 'label'을 포함한 사전\n",
    "        \"\"\"\n",
    "        \n",
    "        # 이미지와 라벨 추출\n",
    "        image, label = sample['image'], sample['label']\n",
    "        \n",
    "        # 이미지를 리사이즈\n",
    "        image = self.resizer(image)\n",
    "        \n",
    "        # 이미지를 PyTorch Tensor로 변환\n",
    "        # to_tensor는 [0, 1] 범위로 이미 정규화해줍니다.\n",
    "        image = to_tensor(image)\n",
    "        \n",
    "        # 라벨을 PyTorch Tensor로 변환 (한글 문자의 유니코드 값으로 변환)\n",
    "        label = torch.tensor(ord(label))\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'label': label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed4a1957-88c2-4e0a-8fb6-fe103a5abf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_FILE = \"./image-data/train-labels.csv\"\n",
    "VALID_CSV_FILE = \"./image-data/train-labels.csv\"\n",
    "ROOT_DIR = \"./image-data/hangul-images\"\n",
    "\n",
    "train_ds = KoreanHandwritingDataset(TRAIN_CSV_FILE, ROOT_DIR, transforms.Compose([ToTensor()]))\n",
    "valid_ds = KoreanHandwritingDataset(TRAIN_CSV_FILE, ROOT_DIR, transforms.Compose([ToTensor()]))\n",
    "\n",
    "train_size = len(train_ds)\n",
    "valid_size = len(valid_ds)\n",
    "print(train_size, valid_size)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle=True, num_workers = NUM_CPU)\n",
    "valid_loader = DataLoader(valid_ds, batch_size = BATCH_SIZE, shuffle=False, num_workers = NUM_CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b832d2c-f0de-403f-bdea-0bafa6916834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [64, 64, 32, 32]           9,408\n",
      "       BatchNorm2d-2           [64, 64, 32, 32]             128\n",
      "              ReLU-3           [64, 64, 32, 32]               0\n",
      "         MaxPool2d-4           [64, 64, 16, 16]               0\n",
      "            Conv2d-5           [64, 64, 16, 16]          36,864\n",
      "       BatchNorm2d-6           [64, 64, 16, 16]             128\n",
      "              ReLU-7           [64, 64, 16, 16]               0\n",
      "            Conv2d-8           [64, 64, 16, 16]          36,864\n",
      "       BatchNorm2d-9           [64, 64, 16, 16]             128\n",
      "             ReLU-10           [64, 64, 16, 16]               0\n",
      "       BasicBlock-11           [64, 64, 16, 16]               0\n",
      "           Conv2d-12           [64, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-13           [64, 64, 16, 16]             128\n",
      "             ReLU-14           [64, 64, 16, 16]               0\n",
      "           Conv2d-15           [64, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-16           [64, 64, 16, 16]             128\n",
      "             ReLU-17           [64, 64, 16, 16]               0\n",
      "       BasicBlock-18           [64, 64, 16, 16]               0\n",
      "           Conv2d-19           [64, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-20           [64, 64, 16, 16]             128\n",
      "             ReLU-21           [64, 64, 16, 16]               0\n",
      "           Conv2d-22           [64, 64, 16, 16]          36,864\n",
      "      BatchNorm2d-23           [64, 64, 16, 16]             128\n",
      "             ReLU-24           [64, 64, 16, 16]               0\n",
      "       BasicBlock-25           [64, 64, 16, 16]               0\n",
      "           Conv2d-26            [64, 128, 8, 8]          73,728\n",
      "      BatchNorm2d-27            [64, 128, 8, 8]             256\n",
      "             ReLU-28            [64, 128, 8, 8]               0\n",
      "           Conv2d-29            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-30            [64, 128, 8, 8]             256\n",
      "           Conv2d-31            [64, 128, 8, 8]           8,192\n",
      "      BatchNorm2d-32            [64, 128, 8, 8]             256\n",
      "             ReLU-33            [64, 128, 8, 8]               0\n",
      "       BasicBlock-34            [64, 128, 8, 8]               0\n",
      "           Conv2d-35            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-36            [64, 128, 8, 8]             256\n",
      "             ReLU-37            [64, 128, 8, 8]               0\n",
      "           Conv2d-38            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-39            [64, 128, 8, 8]             256\n",
      "             ReLU-40            [64, 128, 8, 8]               0\n",
      "       BasicBlock-41            [64, 128, 8, 8]               0\n",
      "           Conv2d-42            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-43            [64, 128, 8, 8]             256\n",
      "             ReLU-44            [64, 128, 8, 8]               0\n",
      "           Conv2d-45            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-46            [64, 128, 8, 8]             256\n",
      "             ReLU-47            [64, 128, 8, 8]               0\n",
      "       BasicBlock-48            [64, 128, 8, 8]               0\n",
      "           Conv2d-49            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-50            [64, 128, 8, 8]             256\n",
      "             ReLU-51            [64, 128, 8, 8]               0\n",
      "           Conv2d-52            [64, 128, 8, 8]         147,456\n",
      "      BatchNorm2d-53            [64, 128, 8, 8]             256\n",
      "             ReLU-54            [64, 128, 8, 8]               0\n",
      "       BasicBlock-55            [64, 128, 8, 8]               0\n",
      "           Conv2d-56            [64, 256, 4, 4]         294,912\n",
      "      BatchNorm2d-57            [64, 256, 4, 4]             512\n",
      "             ReLU-58            [64, 256, 4, 4]               0\n",
      "           Conv2d-59            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-60            [64, 256, 4, 4]             512\n",
      "           Conv2d-61            [64, 256, 4, 4]          32,768\n",
      "      BatchNorm2d-62            [64, 256, 4, 4]             512\n",
      "             ReLU-63            [64, 256, 4, 4]               0\n",
      "       BasicBlock-64            [64, 256, 4, 4]               0\n",
      "           Conv2d-65            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-66            [64, 256, 4, 4]             512\n",
      "             ReLU-67            [64, 256, 4, 4]               0\n",
      "           Conv2d-68            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-69            [64, 256, 4, 4]             512\n",
      "             ReLU-70            [64, 256, 4, 4]               0\n",
      "       BasicBlock-71            [64, 256, 4, 4]               0\n",
      "           Conv2d-72            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-73            [64, 256, 4, 4]             512\n",
      "             ReLU-74            [64, 256, 4, 4]               0\n",
      "           Conv2d-75            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-76            [64, 256, 4, 4]             512\n",
      "             ReLU-77            [64, 256, 4, 4]               0\n",
      "       BasicBlock-78            [64, 256, 4, 4]               0\n",
      "           Conv2d-79            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-80            [64, 256, 4, 4]             512\n",
      "             ReLU-81            [64, 256, 4, 4]               0\n",
      "           Conv2d-82            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-83            [64, 256, 4, 4]             512\n",
      "             ReLU-84            [64, 256, 4, 4]               0\n",
      "       BasicBlock-85            [64, 256, 4, 4]               0\n",
      "           Conv2d-86            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-87            [64, 256, 4, 4]             512\n",
      "             ReLU-88            [64, 256, 4, 4]               0\n",
      "           Conv2d-89            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-90            [64, 256, 4, 4]             512\n",
      "             ReLU-91            [64, 256, 4, 4]               0\n",
      "       BasicBlock-92            [64, 256, 4, 4]               0\n",
      "           Conv2d-93            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-94            [64, 256, 4, 4]             512\n",
      "             ReLU-95            [64, 256, 4, 4]               0\n",
      "           Conv2d-96            [64, 256, 4, 4]         589,824\n",
      "      BatchNorm2d-97            [64, 256, 4, 4]             512\n",
      "             ReLU-98            [64, 256, 4, 4]               0\n",
      "       BasicBlock-99            [64, 256, 4, 4]               0\n",
      "          Conv2d-100            [64, 512, 2, 2]       1,179,648\n",
      "     BatchNorm2d-101            [64, 512, 2, 2]           1,024\n",
      "            ReLU-102            [64, 512, 2, 2]               0\n",
      "          Conv2d-103            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-104            [64, 512, 2, 2]           1,024\n",
      "          Conv2d-105            [64, 512, 2, 2]         131,072\n",
      "     BatchNorm2d-106            [64, 512, 2, 2]           1,024\n",
      "            ReLU-107            [64, 512, 2, 2]               0\n",
      "      BasicBlock-108            [64, 512, 2, 2]               0\n",
      "          Conv2d-109            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-110            [64, 512, 2, 2]           1,024\n",
      "            ReLU-111            [64, 512, 2, 2]               0\n",
      "          Conv2d-112            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-113            [64, 512, 2, 2]           1,024\n",
      "            ReLU-114            [64, 512, 2, 2]               0\n",
      "      BasicBlock-115            [64, 512, 2, 2]               0\n",
      "          Conv2d-116            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-117            [64, 512, 2, 2]           1,024\n",
      "            ReLU-118            [64, 512, 2, 2]               0\n",
      "          Conv2d-119            [64, 512, 2, 2]       2,359,296\n",
      "     BatchNorm2d-120            [64, 512, 2, 2]           1,024\n",
      "            ReLU-121            [64, 512, 2, 2]               0\n",
      "      BasicBlock-122            [64, 512, 2, 2]               0\n",
      "AdaptiveAvgPool2d-123            [64, 512, 1, 1]               0\n",
      "          Linear-124                   [64, 10]           5,130\n",
      "================================================================\n",
      "Total params: 21,289,802\n",
      "Trainable params: 21,289,802\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 503.25\n",
      "Params size (MB): 81.21\n",
      "Estimated Total Size (MB): 587.47\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 사전학습된 가중치를 가져오지 않도록 pretrained는 Fasle\r\n",
    "model = torchvision.models.resnet34(pretrained = False)\r\n",
    "\r\n",
    "# number of features in the input of the linear layer\r\n",
    "num_ftrs = model.fc.in_features\r\n",
    "\r\n",
    "# sets the number of features of the linear layer\r\n",
    "model.fc = torch.nn.Linear(num_ftrs, NUM_CLASSES)\r\n",
    "model = model.to(DEVICE)\r\n",
    "\r\n",
    "# parameters\r\n",
    "criterion = torch.nn.CrossEntropyLoss()\r\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = LEANING_RATE)\r\n",
    "\r\n",
    "\r\n",
    "## model summary\r\n",
    "summary(model, (3, IMG_SIZE[0], IMG_SIZE[1]), BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab965d5d-4a3a-4ec0-ab1b-fc77bdc0155a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "# function to train model\n",
    "def train_model(model, criterion, optimizer, num_epochs, train_loader, val_loader):\n",
    "    since = time.time()\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        \n",
    "        # train\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for step, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        # validate\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / val_size\n",
    "        epoch_acc = running_corrects.double() / val_size\n",
    "        print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        print('-' * 30)\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val Acc: {:.4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n",
    "\n",
    "# train the model\n",
    "model = train_model(model, criterion, optimizer, NUM_EPOCHS, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b46dc7-2053-4fe4-ac6a-e59a63292656",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
