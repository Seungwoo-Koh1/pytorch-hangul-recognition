{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112d94dd-8c08-445e-a0e5-5329338197f6",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2ba32c-3122-4551-9674-6a72a326dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *------- Basic setup -------*\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, random, time\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# *------- torch -------*\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# *------- albumentations -------*\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "# *------- sklearn -------*\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb29fe47-6cb9-4141-b4e6-1d8fa6c0ccc7",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1027188-9b09-48f9-998d-371a2ee3e61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "IMG_SIZE = (64, 64)\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_CLASSES = 256\n",
    "NUM_EPOCHS = 100\n",
    "NUM_CPU = cpu_count()\n",
    "\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726a9806-eb03-4910-ac16-3092c97c6788",
   "metadata": {},
   "source": [
    "## Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61804326-c135-4b2e-a689-3853af94ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(csv_path, train_save_path, valid_save_path, test_size=0.2, random_seed=SEED):\n",
    "    \"\"\"\n",
    "    주어진 CSV 파일을 train과 val 데이터셋으로 나누어 저장하는 함수.\n",
    "\n",
    "    Args:\n",
    "    - csv_path: 입력 CSV 파일 경로\n",
    "    - train_save_path: 학습 데이터셋 저장 경로\n",
    "    - valid_save_path: 검증 데이터셋 저장 경로\n",
    "    - test_size: 검증 데이터셋의 비율 (default: 0.2)\n",
    "    - random_seed: 데이터 분할시 사용되는 랜덤 시드 (default: 42)\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    \n",
    "    # CSV 파일 읽기\n",
    "    df = pd.read_csv(csv_path, header=None, names=['path', 'label'])\n",
    "\n",
    "    # 데이터를 train과 val로 나누기\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=random_seed, stratify=df['label'])\n",
    "\n",
    "    # 나눠진 데이터를 CSV 파일로 저장\n",
    "    train_df.to_csv(train_save_path, index=False, header=False)\n",
    "    valid_df.to_csv(valid_save_path, index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf93f1d-88f8-445b-a500-3dcbdeec5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset('image-data/labels-map.csv', 'image-data/train-labels.csv', 'image-data/valid-labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29a57f0-19aa-404f-8d12-6ba5b064d5bc",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644491c-153d-4f8d-a826-f0b9eff338ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanHandwritingDataset(Dataset):\n",
    "    \"\"\"\n",
    "    한글 손글씨 데이터셋을 로드하고 처리하기 위한 클래스.\n",
    "\n",
    "    Attributes:\n",
    "    - dataset (DataFrame): CSV 파일에서 읽어들인 데이터.\n",
    "    - root_dir (str): 이미지 파일이 저장된 기본 디렉터리 경로.\n",
    "    - transform (callable, optional): 샘플에 적용될 변환 (예: 데이터 증강).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_file, root_dir, label_file, transform=None):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - csv_file (str): CSV 파일의 경로.\n",
    "        - root_dir (str): 모든 이미지가 저장된 디렉터리 경로.\n",
    "        - label_file (str) :\n",
    "        - transform (callable, optional): 샘플에 적용할 선택적 변환.\n",
    "        \"\"\"\n",
    "        self.dataset = pd.read_csv(csv_file, header=None, names=['path', 'label'])\n",
    "        self.root_dir = root_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transforms\n",
    "\n",
    "        # 파일에서 한글 글자를 읽어오기\n",
    "        with open(self.label_file, 'r', encoding='utf-8') as f:\n",
    "            hangul_chars = [line.strip() for line in f.readlines()]\n",
    "        \n",
    "        # 각 한글 글자에 순차적으로 레이블 번호 부여\n",
    "        self.label_mapping = {char: idx for idx, char in enumerate(hangul_chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        데이터셋의 길이를 반환합니다.\n",
    "        \n",
    "        Returns:\n",
    "        - int: 데이터셋 내의 샘플 수.\n",
    "        \"\"\"\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        주어진 인덱스에 해당하는 샘플을 반환합니다.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx (int): 반환할 샘플의 인덱스.\n",
    "\n",
    "        Returns:\n",
    "        - dict: 'image' 및 'label' 키를 포함하는 사전.\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.root_dir, self.dataset.iloc[idx]['path'])\n",
    "        # Gray 이미지를 RGB로 열기\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img /= 255\n",
    "        \n",
    "        if self.transforms:\n",
    "            img = self.transform(image=img)['image']\n",
    "                \n",
    "        label = self.dataset.iloc[idx]['label']\n",
    "        label = self.label_mapping[label]\n",
    "        label = torch.tensor(label, dtype=torch.int64)\n",
    "        return img, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a1957-88c2-4e0a-8fb6-fe103a5abf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_FILE = \"./image-data/train-labels.csv\"\n",
    "VALID_CSV_FILE = \"./image-data/valid-labels.csv\"\n",
    "LABEL_FILE = \"./labels/256-common-hangul.txt\"\n",
    "ROOT_DIR = \"./image-data/hangul-images\"\n",
    "\n",
    "train_dataset = KoreanHandwritingDataset(TRAIN_CSV_FILE, ROOT_DIR, LABEL_FILE)\n",
    "valid_dataset = KoreanHandwritingDataset(VALID_CSV_FILE, ROOT_DIR, LABEL_FILE)\n",
    "\n",
    "train_size = len(train_dataset)\n",
    "valid_size = len(valid_dataset)\n",
    "print(train_size, valid_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be87ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 채널 별 mean 계산\n",
    "def get_mean(dataset):\n",
    "  meanRGB = [np.mean(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
    "  meanR = np.mean([m[0] for m in meanRGB])\n",
    "  meanG = np.mean([m[1] for m in meanRGB])\n",
    "  meanB = np.mean([m[2] for m in meanRGB])\n",
    "  return [meanR, meanG, meanB]\n",
    "\n",
    "# 채널 별 str 계산\n",
    "def get_std(dataset):\n",
    "  stdRGB = [np.std(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
    "  stdR = np.mean([s[0] for s in stdRGB])\n",
    "  stdG = np.mean([s[1] for s in stdRGB])\n",
    "  stdB = np.mean([s[2] for s in stdRGB])\n",
    "  return [stdR, stdG, stdB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea21d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = transforms.Compose(\n",
    "    [transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(get_mean(train_dataset), get_std(train_dataset))]\n",
    "    )\n",
    "valid_transforms = transforms.Compose(\n",
    "    [transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(get_mean(test_dataset), get_std(test_dataset))]\n",
    "    )\n",
    "\n",
    "train_dataset.transform = train_transforms\n",
    "valid_dataset.transform = valid_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fad076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers = NUM_CPU)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=False, num_workers = NUM_CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9c0b7",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be92f1-a4ed-4e9b-a46b-d92afa849af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전학습된 가중치를 가져오도록 pretrained는 True로 설정\n",
    "model = models.resnet34(pretrained=True)\n",
    "\n",
    "# number of features in the input of the linear layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# sets the number of features of the linear layer\n",
    "model.fc = torch.nn.Linear(num_ftrs, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# parameters\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "summary(model, (3, 64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수 정의\n",
    "def train_model(model, criterion, optimizer, num_epochs, train_loader, val_loader, save_dir='model_best.pth'):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        \n",
    "        # 학습 모드 설정\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for step, (inputs, labels) in enumerate(tqdm(train_loader, desc=\"train\")):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / train_size\n",
    "        epoch_acc = running_corrects.double() / train_size\n",
    "        print('Train Loss: {:.4f} Train Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "\n",
    "        # 검증 모드 설정\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        for inputs, labels in tqdm(val_loader, desc=\"valid\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "        epoch_loss = running_loss / valid_size\n",
    "        epoch_acc = running_corrects.double() / valid_size\n",
    "        print('Val Loss: {:.4f} Val Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        print('-' * 30)\n",
    "        if epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            # 최적의 모델 가중치 저장\n",
    "            torch.save(best_model, save_dir)\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best Val Acc: {:.4f}'.format(best_acc))\n",
    "    model.load_state_dict(best_model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8676d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train the model\n",
    "model = train_model(model, criterion, optimizer, NUM_EPOCHS, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33ba3c-522a-4781-b6c5-dde45f4b367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict_image(model, image_path, transform):\n",
    "    # 이미지 불러오기\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # 이미지 전처리\n",
    "    image = transform(image).unsqueeze(0) # 차원 추가 (배치 차원)\n",
    "    image = image.to(DEVICE)\n",
    "    \n",
    "    # 모델 추론 모드 설정 및 예측\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        return predicted.item()\n",
    "\n",
    "# ResNet34의 입력 사이즈와 일치하도록 이미지 전처리를 위한 transform 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),  # ResNet34 입력 사이즈에 맞춰 이미지 크기 조정\n",
    "    transforms.ToTensor(),\n",
    "    # 만약 추가 전처리가 필요하면 이곳에 추가\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901d412d-d1cb-46c8-9699-b08b92b56057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 수행\n",
    "image_path = './test-images/test_1.jpeg'\n",
    "predicted_label = predict_image(model, image_path, transform)\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2735bd-8314-40fa-8813-cf171ae67372",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './test-images/test_2.jpg'\n",
    "predicted_label = predict_image(model, image_path, transform)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7d4d35-96de-4222-a754-79cff3dd35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './test-images/test_3.jpg'\n",
    "predicted_label = predict_image(model, image_path, transform)\n",
    "print(f\"Predicted Label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbd0907-7941-4951-a57e-16128bee9cfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
