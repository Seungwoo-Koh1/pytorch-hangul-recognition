{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c6635-c2c6-407e-bc0a-2df5ec5d356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yeongseonchoe/GitHub/pytorch-hangul-recognition/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/yeongseonchoe/GitHub/pytorch-hangul-recognition/venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "  0%|                                                                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/0\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "# Seed 설정 함수\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 데이터셋 분리 함수\n",
    "def split_dataset(csv_path, train_save_path, valid_save_path, test_size, random_seed):\n",
    "    df = pd.read_csv(csv_path, header=None, names=['path', 'label'])\n",
    "    train_df, valid_df = train_test_split(df, test_size=test_size, random_state=random_seed, stratify=df['label'])\n",
    "    train_df.to_csv(train_save_path, index=False)\n",
    "    valid_df.to_csv(valid_save_path, index=False)\n",
    "\n",
    "# 데이터셋 클래스\n",
    "class KoreanHandwritingDataset(Dataset):\n",
    "    def __init__(self, csv_file, image_dir, label_file, transform=None):\n",
    "        self.dataset = pd.read_csv(csv_file, header=None, names=['path', 'label'])\n",
    "        self.image_dir = image_dir\n",
    "        self.label_file = label_file\n",
    "        self.transform = transform\n",
    "        with open(self.label_file, 'r', encoding='utf-8') as f:\n",
    "            hangul_chars = [line.strip() for line in f.readlines()]\n",
    "        self.label_mapping = {char: idx for idx, char in enumerate(hangul_chars)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.dataset.iloc[idx]['path'])\n",
    "        # 그레이스케일로 이미지 열기\n",
    "        image = Image.open(img_path).convert(\"L\")\n",
    "        label = self.dataset.iloc[idx]['label']\n",
    "        label = self.label_mapping[label]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 데이터 변환\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # 그레이스케일 이미지를 3채널로 복제\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # 각 채널에 대한 평균 및 표준편차\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=3),  # 동일한 처리\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 데이터셋 및 데이터 로더 준비\n",
    "# CSV 파일 분리\n",
    "split_dataset('image-data/labels-map.csv', 'image-data/train-labels.csv', 'image-data/valid-labels.csv', 0.2, 42)\n",
    "\n",
    "train_csv_file = \"./image-data/train-labels.csv\"\n",
    "valid_csv_file = \"./image-data/valid-labels.csv\"\n",
    "label_file = \"./labels/256-common-hangul.txt\"\n",
    "image_dir = \"./image-data/hangul-images\"\n",
    "\n",
    "train_dataset = KoreanHandwritingDataset(train_csv_file, image_dir, label_file, train_transform)\n",
    "valid_dataset = KoreanHandwritingDataset(valid_csv_file, image_dir, label_file, valid_transform)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "# 장치 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 준비\n",
    "model_ft = models.efficientnet_b0(pretrained=True)\n",
    "num_ftrs = model_ft.classifier[1].in_features\n",
    "model_ft.classifier[1] = nn.Linear(num_ftrs, 256)\n",
    "#model_ft = models.vgg16(pretrained=True)\n",
    "#model_ft.classifier[6] = nn.Linear(in_features=4096, out_features=256)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "# 손실 함수 및 최적화 함수 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "# 훈련 함수\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # 각 에포크는 학습 단계와 검증 단계를 갖습니다.\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # 모델을 학습 모드로 설정\n",
    "            else:\n",
    "                model.eval()   # 모델을 평가 모드로 설정\n",
    "                \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # 데이터를 배치 단위로 가져와 처리합니다.\n",
    "            for inputs, labels in data_loader[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # 파라미터 경사도를 0으로 설정\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # 순전파\n",
    "                # 학습 시에만 연산 기록을 추적\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # 학습 단계인 경우 역전파 + 최적화\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "                # 통계\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "            \n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader[phase].dataset)\n",
    "            \n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            \n",
    "            # 모델을 깊은 복사(deep copy)함\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # 가장 나은 모델 가중치를 불러옴\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "data_loader = {'train': train_data_loader, 'valid': valid_data_loader}\n",
    "\n",
    "# 모델 훈련\n",
    "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c85e50b-4415-48e0-9078-094c3e67e210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
